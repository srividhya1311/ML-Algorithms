{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJPasqbwc_VA"
   },
   "outputs": [],
   "source": [
    "#file_path for the weights of the trained model to be used\n",
    "#https://polybox.ethz.ch/index.php/s/ibQHQSyQsqBKjCl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sR3lxjvUlrTb"
   },
   "outputs": [],
   "source": [
    "#cells where file paths need to be altered to rerun \n",
    "#Loc-1: Read train.txt\n",
    "#Loc-2: Split into training and validation sets and save as txt\n",
    "#Loc-3: File paths for training,validation,test,all_images \n",
    "#Loc-4: Function to read the pixel data of each image and convert it to target size\n",
    "#Loc-5: File path for loading trained weights (polybox reference path provided at the top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "6PGLleUYDbSd",
    "outputId": "4ef79afb-ede2-4aec-8570-9adcec364a08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "M-eOWHS6yXV2",
    "outputId": "99417bf6-c391-4d25-b1d4-0261118df844"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-2.2.0:\n",
      "  Would remove:\n",
      "    /usr/local/bin/estimator_ckpt_converter\n",
      "    /usr/local/bin/saved_model_cli\n",
      "    /usr/local/bin/tensorboard\n",
      "    /usr/local/bin/tf_upgrade_v2\n",
      "    /usr/local/bin/tflite_convert\n",
      "    /usr/local/bin/toco\n",
      "    /usr/local/bin/toco_from_protos\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled tensorflow-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "colab_type": "code",
    "id": "mhvk3vihya8J",
    "outputId": "fda4b19a-0482-4e7d-8570-0c6d67dc40ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu==2.0.0-alpha0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
      "\u001b[K     |████████████████████████████████| 332.1MB 38kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.34.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
      "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
      "\u001b[K     |████████████████████████████████| 419kB 40.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.3.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.8)\n",
      "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 39.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.29.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.18.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (47.1.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.0)\n",
      "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow-gpu\n",
      "Successfully installed tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "S9FWhiFIuMhM",
    "outputId": "4008cc99-1952-40c1-a2b9-d673f7b899ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import image\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from glob import glob\n",
    "from keras.layers import Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1GGs915wuZ8d"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.python.compiler import tensorrt as trt\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sq0a_rLkJOBq"
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from random import choice, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgZSKwPVgkW0"
   },
   "outputs": [],
   "source": [
    "#Loc-1: read training txt\n",
    "train_path = \"drive/My Drive/Task 4/train.txt\"\n",
    "train=pd.read_csv(train_path,header=None)\n",
    "#Loc-2: split into training and validation sets and save as txt\n",
    "X_train,X_val=train_test_split(train,test_size=0.25, random_state=1)\n",
    "np.savetxt(\"drive/My Drive/Task 4/X_train.txt\", X_train, delimiter=\",\", fmt='%s')\n",
    "np.savetxt(\"drive/My Drive/Task 4/X_val.txt\", X_val, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EVLJZeT1ud6X"
   },
   "outputs": [],
   "source": [
    "#Loc-3: File paths for training, validation,test txts,images path\n",
    "train_file_path = \"drive/My Drive/Task 4/X_train.txt\"\n",
    "val_file_path = \"drive/My Drive/Task 4/X_val.txt\"\n",
    "test_file_path=\"drive/My Drive/Task 4/test.txt\"\n",
    "images_path = \"drive/My Drive/Task 4/Food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYpUuSEZA-db"
   },
   "outputs": [],
   "source": [
    "#read training, validation and test txt\n",
    "train_triplets=pd.read_csv(train_file_path,header=None) \n",
    "val_triplets=pd.read_csv(val_file_path,header=None)\n",
    "test_triplets=pd.read_csv(test_file_path,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRrvyGpxDWEc"
   },
   "outputs": [],
   "source": [
    "#reading triplet txts to save anchor, positive and negative image filenames\n",
    "train_anchor=[]\n",
    "train_pos=[]\n",
    "train_neg=[]\n",
    "for i in range(0,44636):\n",
    "  x=str(train_triplets.values[i][0])\n",
    "  img1=x[0:5]\n",
    "  train_anchor.append(img1)\n",
    "  img2=x[6:11]\n",
    "  train_pos.append(img2)\n",
    "  img3=x[12:17]\n",
    "  train_neg.append(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTQ3xb0NPkPU"
   },
   "outputs": [],
   "source": [
    "val_anchor=[]\n",
    "val_pos=[]\n",
    "val_neg=[]\n",
    "for i in range(0,14879):\n",
    "  x=str(val_triplets.values[i][0])\n",
    "  img1=x[0:5]\n",
    "  val_anchor.append(img1)\n",
    "  img2=x[6:11]\n",
    "  val_pos.append(img2)\n",
    "  img3=x[12:17]\n",
    "  val_neg.append(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-jbbxWDREB6"
   },
   "outputs": [],
   "source": [
    "test_anchor=[]\n",
    "test_pos=[]\n",
    "test_neg=[]\n",
    "for i in range(0,59544):\n",
    "  x=str(test_triplets.values[i][0])\n",
    "  img1=x[0:5]\n",
    "  test_anchor.append(img1)\n",
    "  img2=x[6:11]\n",
    "  test_pos.append(img2)\n",
    "  img3=x[12:17]\n",
    "  test_neg.append(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dau1RLDQP5_X"
   },
   "outputs": [],
   "source": [
    "train_tuples=list(zip(train_anchor,train_pos,train_neg))\n",
    "val_tuples=list(zip(val_anchor,val_pos,val_neg))\n",
    "test_tuples=list(zip(test_anchor,test_pos,test_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_9_xRVtQYu3"
   },
   "outputs": [],
   "source": [
    "#Loc-4:function to read the pixel data of each image and convert it to target size\n",
    "def read_img(img_x):\n",
    "    path=\"drive/My Drive/Task 4/food/\"+img_x+'.jpg'\n",
    "    img = image.load_img(path, target_size=(225,225))\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4duSzm1SSV9"
   },
   "outputs": [],
   "source": [
    "#generating tuples\n",
    "def gen(list_tuples,batch_size=120):\n",
    "  while True:\n",
    "    batch_tuples=sample(list_tuples,batch_size)\n",
    "    labels=[1]*len(batch_tuples)\n",
    "    X1=[x[0] for x in batch_tuples]\n",
    "    X1=np.array([read_img(x) for x in X1])\n",
    "    X2=[x[1] for x in batch_tuples]\n",
    "    X2=np.array([read_img(x) for x in X2])\n",
    "    X3=[x[2] for x in batch_tuples]\n",
    "    X3=np.array([read_img(x) for x in X3])\n",
    "    yield [X1,X2,X3],labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ah4s-BLKXyCK"
   },
   "outputs": [],
   "source": [
    "#defining triplet loss\n",
    "def triplet_loss(y_true,y_pred,alpha=0.25):\n",
    "  total_length = y_pred.shape.as_list()[-1]\n",
    "  anchor=y_pred[:,0:int(total_length*1/3)]\n",
    "  positive=y_pred[:,int(total_length*1/3):int(total_length*2/3)]\n",
    "  negative=y_pred[:,int(total_length*2/3):int(total_length*3/3)]\n",
    "  pos_dist=K.sum(K.square(anchor-positive),axis=1)\n",
    "  neg_dist=K.sum(K.square(anchor-negative),axis=1)\n",
    "  basic_loss=pos_dist-neg_dist+alpha\n",
    "  loss=K.maximum(basic_loss,0.0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_ny2cvkJi0n"
   },
   "outputs": [],
   "source": [
    "#defining label prediction based on triplet loss\n",
    "def triplet_loss_label(y_pred):\n",
    "  total_length = len(y_pred)\n",
    "  anchor=np.array(y_pred[0:int(total_length*1/3)])\n",
    "  positive=np.array(y_pred[int(total_length*1/3):int(total_length*2/3)])\n",
    "  negative=np.array(y_pred[int(total_length*2/3):int(total_length*3/3)])\n",
    "  pos_dist=np.sum(np.square(anchor-positive))\n",
    "  neg_dist=np.sum(np.square(anchor-negative))\n",
    "  basic_loss=pos_dist-neg_dist\n",
    "  if basic_loss>=0:\n",
    "    label=0\n",
    "  else:\n",
    "    label=1  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hW6gCYz3L3-j"
   },
   "outputs": [],
   "source": [
    "def base_model(input_vgg):\n",
    "  vgg=VGG16(input_tensor=input_vgg,weights='imagenet', include_top=False)\n",
    "  for x in vgg.layers:\n",
    "    x.trainable=False\n",
    "  out=vgg.output\n",
    "  v_x1=Conv2D(96,kernel_size=(6),strides=(6),padding=\"same\")(out)\n",
    "  v_x2=MaxPool2D()(v_x1)\n",
    "  v_x3=Dropout(0.6)(v_x2)\n",
    "  #final=Dense(64,activation='sigmoid')(x3)\n",
    "  base_model=Model(input_vgg,v_x3)\n",
    "  return base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YgjUMtHtY1LP"
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "  input_1=Input(shape=(225,225,3))\n",
    "  input_2=Input(shape=(225,225,3))\n",
    "  input_3=Input(shape=(225,225,3))\n",
    "  vgg=VGG16(weights='imagenet', include_top=False)\n",
    "  for x in vgg.layers:\n",
    "    x.trainable=False\n",
    "  base_model=Sequential()\n",
    "  for x in vgg.layers:\n",
    "    base_model.add(x)\n",
    "  #base_model.add(Conv2D(64,kernel_size=(6),strides=(6),padding=\"same\"))\n",
    "  #base_model.add(Dropout(0.6))\n",
    "  base_model.add(Dense(64,activation='sigmoid'))\n",
    "  base_model.add(Dense(32,activation='sigmoid'))\n",
    "  base_model.add(Dense(16,activation='sigmoid'))\n",
    "  #base_model.add(Dense(8,activation='sigmoid'))\n",
    "  x1=base_model(input_1)\n",
    "  x2=base_model(input_2)\n",
    "  x3=base_model(input_3)\n",
    "  f_x1=Concatenate(axis=-1)([GlobalMaxPool2D()(x1),GlobalAvgPool2D()(x1)])\n",
    "  f_x2=Concatenate(axis=-1)([GlobalMaxPool2D()(x2),GlobalAvgPool2D()(x2)])\n",
    "  f_x3=Concatenate(axis=-1)([GlobalMaxPool2D()(x3),GlobalAvgPool2D()(x3)])\n",
    "  concat_vector=concatenate([f_x1,f_x2,f_x3],axis=-1,name='concat')\n",
    "  model=Model([input_1,input_2,input_3],concat_vector)\n",
    "  model.compile(loss=triplet_loss,optimizer=Adam(0.0001))\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 773
    },
    "colab_type": "code",
    "id": "qXeneLRIaIjZ",
    "outputId": "294f0b41-4452-4ea1-f79d-0252fabb1934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 225, 225, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 225, 225, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 225, 225, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             14750128    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 16)           0           sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 16)           0           sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 16)           0           sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 16)           0           sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 16)           0           sequential_1[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 16)           0           sequential_1[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32)           0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32)           0           global_max_pooling2d_2[0][0]     \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32)           0           global_max_pooling2d_3[0][0]     \n",
      "                                                                 global_average_pooling2d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 96)           0           concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 14,750,128\n",
      "Trainable params: 35,440\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IxUYTP4xMgCV",
    "outputId": "c4bb0088-a6e0-40fd-d0c5-f39f0e7c0ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:718: UserWarning: An input could not be retrieved. It could be because a worker has died.We do not have any information on the lost sample.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 3804s 152s/step - loss: 0.2492\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 78s 3s/step - loss: 0.2489\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 73s 3s/step - loss: 0.2483\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 72s 3s/step - loss: 0.2468\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.2451\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 72s 3s/step - loss: 0.2413\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.2357\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.2251\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.2140\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.2021\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.1934\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1890\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 71s 3s/step - loss: 0.1946\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1935\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1876\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1916\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1910\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1926\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1879\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1854\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1857\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1918\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1812\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1837\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1882\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1837\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1819\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1851\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1820\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1809\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1769\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1814\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1792\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1714\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1780\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1726\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1760\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1737\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1799\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1719\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1746\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1757\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1745\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1780\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1680\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1761\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1737\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1754\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1757\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1740\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1720\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1702\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1712\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1698\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1669\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1722\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1641\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1688\n",
      "Epoch 59/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1693\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1725\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1670\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1636\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1633\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1682\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1603\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1734\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1681\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1757\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1685\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1688\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 70s 3s/step - loss: 0.1691\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1669\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1620\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1659\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1668\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1648\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1638\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1644\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1613\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1553\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1651\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1695\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1698\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1658\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1646\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1610\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1599\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1625\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1617\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1575\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1629\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1531\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1633\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1674\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1669\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1640\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1598\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1597\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1614\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 69s 3s/step - loss: 0.1584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa067f1fa90>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit_generator(gen(train_tuples,batch_size=100),use_multiprocessing=True,\n",
    "#                  validation_data=gen(val_tuples,batch_size=50),epochs=100,verbose=1,steps_per_epoch=25,validation_steps=10)\n",
    "model.fit_generator(gen(train_tuples,batch_size=100),use_multiprocessing=True,\n",
    "                    epochs=100,verbose=1,steps_per_epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmMeQ4yMbHN4"
   },
   "outputs": [],
   "source": [
    "#file_path for saving weights from each run\n",
    "#file_path=\"drive/My Drive/Task 4/triplet.h5\"\n",
    "#model.save_weights(file_path)\n",
    "#\n",
    "#Loc-5:file path for loading trained weights (polybox reference path provided at the top)\n",
    "file_path=\"drive/My Drive/Task 4/triplet_trained.h5\"\n",
    "model.load_weights(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3H4GyvPX-3TN"
   },
   "outputs": [],
   "source": [
    "#making predictions for test triplets\n",
    "predictions=list()\n",
    "for i in range(0,59544):\n",
    "  X1=np.array([read_img(test_anchor[i])])\n",
    "  X2=np.array([read_img(test_pos[i])])\n",
    "  X3=np.array([read_img(test_neg[i])])\n",
    "  dist=model.predict([X1,X2,X3]).ravel().tolist()\n",
    "  label=triplet_loss_label(dist)\n",
    "  predictions.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jiUKwFPwaWx"
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"pred_trained_weights.csv\", predictions, delimiter=\",\", fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final_code_trained_weights.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
